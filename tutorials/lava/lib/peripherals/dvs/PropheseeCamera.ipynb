{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df6eded-4456-406f-b5b9-bab9bc7c687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/metavision_core_ml/preprocessing/viz.py:41: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _viz_events(events, img):\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "from lava.magma.core.run_configs import Loihi2SimCfg, Loihi2HwCfg\n",
    "from lava.magma.core.run_conditions import RunSteps, RunContinuous\n",
    "from lava.magma.core.decorator import implements, requires, tag\n",
    "from lava.magma.core.model.py.model import PyLoihiProcessModel\n",
    "from lava.magma.core.model.py.ports import PyOutPort, PyInPort\n",
    "from lava.magma.core.model.py.type import LavaPyType\n",
    "from lava.magma.core.process.ports.ports import OutPort, InPort\n",
    "from lava.magma.core.process.process import AbstractProcess\n",
    "from lava.magma.core.process.variable import Var\n",
    "from lava.magma.core.resources import CPU\n",
    "from lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\n",
    "from lava.proc.io.sink import RingBuffer\n",
    "\n",
    "from lava.lib.peripherals.dvs.process import PropheseeCamera\n",
    "from metavision_core.utils import get_sample\n",
    "from metavision_sdk_cv import TrailFilterAlgorithm, ActivityNoiseFilterAlgorithm\n",
    "from metavision_sdk_core import PolarityFilterAlgorithm\n",
    "\n",
    "from multiprocessing import Pipe\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import IPython\n",
    "\n",
    "\n",
    "from metavision_core.event_io import RawReader\n",
    "from lava.lib.peripherals.dvs.transform import Compose, MirrorHorizontally, MirrorVertically, MergePolarities, Downsample\n",
    "from metavision_ml.preprocessing import histo, histo_quantized\n",
    "from metavision_core.event_io import RawReader\n",
    "\n",
    "from lava.proc.embedded_io.spike import PyToNxAdapter, NxToPyAdapter\n",
    "from lava.proc.lif.process import LIF, LogConfig\n",
    "from lava.proc.dense.process import Dense\n",
    "from lava.proc.sparse.process import Sparse\n",
    "import logging\n",
    "from lava.proc.dense.models import PyDenseModelBitAcc\n",
    "from lava.proc.lif.models import PyLifModelBitAcc\n",
    "from lava.proc.sparse.models import PySparseModelBitAcc\n",
    "\n",
    "from utils import VisProcess, VisUpDownProcess\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e456b29-9503-4546-adc7-37c8d511dcf3",
   "metadata": {},
   "source": [
    "\n",
    "## Download raw event recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602de9c0-4501-42b3-b604-589346b82b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_FILENAME_RAW = \"traffic_monitoring.raw\"\n",
    "# if the file doesn't exist, it will be downloaded from Prophesee's public sample server\n",
    "get_sample(SEQUENCE_FILENAME_RAW)\n",
    "\n",
    "reader = RawReader(SEQUENCE_FILENAME_RAW)\n",
    "height, width = reader.get_size()\n",
    "del reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e72821b-418a-451d-a52d-1e33f96faccb",
   "metadata": {},
   "source": [
    "## Basic usage\n",
    "\n",
    "Initialize PropheseeCamera using the path to the recording, in order to use a live camera, replace the path with an empty string (\"\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce2d8f2-a440-4d4f-a86b-b0b62d3fbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Init Processes\n",
    "frame_input = PropheseeCamera(device=SEQUENCE_FILENAME_RAW,\n",
    "                              sensor_shape=(height, width),\n",
    "                              num_output_time_bins=1)\n",
    "\n",
    "recv = VisProcess(shape=frame_input.s_out.shape)\n",
    "\n",
    "# Connect\n",
    "frame_input.s_out.connect(recv.s_in)\n",
    "\n",
    "# Run\n",
    "num_steps = 200\n",
    "run_cfg = Loihi2SimCfg()\n",
    "run_cnd = RunSteps(num_steps=num_steps)\n",
    "\n",
    "frame_input.run(condition=run_cnd, run_cfg=run_cfg)\n",
    "frame_input.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a80a0-3d5f-4317-9a95-14b121bbf1d5",
   "metadata": {},
   "source": [
    "## Apply filters\n",
    "\n",
    "As you see, the falling items in the recording have a trail. In order to remove the trails, we can apply the TrailFilterAlgorithm from the metavision_sdk. See the metavision documentation for more filters and their usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ef5962-56de-411c-ba02-2113163703d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [TrailFilterAlgorithm(width=width, height=height, threshold=100000),\n",
    "           ActivityNoiseFilterAlgorithm(width=width, height=height, threshold=1000),]\n",
    "\n",
    "\n",
    "# Init Processes\n",
    "frame_input = PropheseeCamera(device=SEQUENCE_FILENAME_RAW,\n",
    "                              filters=filters,\n",
    "                              sensor_shape=(height, width),\n",
    "                              num_output_time_bins=1)\n",
    "\n",
    "recv = VisProcess(shape=frame_input.s_out.shape)\n",
    "\n",
    "# Connect\n",
    "frame_input.s_out.connect(recv.s_in)\n",
    "\n",
    "# Run\n",
    "num_steps = 200\n",
    "run_cfg = Loihi2SimCfg()\n",
    "run_cnd = RunSteps(num_steps=num_steps)\n",
    "\n",
    "frame_input.run(condition=run_cnd, run_cfg=run_cfg)\n",
    "frame_input.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebade859-eaca-4b07-a6df-3ecf4136df62",
   "metadata": {},
   "source": [
    "## Apply transformations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c87662b-1fe1-4594-bcb0-2a30da9bf41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = Compose(\n",
    "    [\n",
    "        Downsample(factor=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Init Processes\n",
    "frame_input = PropheseeCamera(device=SEQUENCE_FILENAME_RAW,\n",
    "                              transformations=transformations,\n",
    "                              sensor_shape=(height, width),\n",
    "                              num_output_time_bins=1)\n",
    "\n",
    "recv = VisProcess(shape=frame_input.s_out.shape)\n",
    "\n",
    "# Connect\n",
    "frame_input.s_out.connect(recv.s_in)\n",
    "\n",
    "# Run\n",
    "num_steps = 200\n",
    "run_cfg = Loihi2SimCfg()\n",
    "run_cnd = RunSteps(num_steps=num_steps)\n",
    "\n",
    "frame_input.run(condition=run_cnd, run_cfg=run_cfg)\n",
    "frame_input.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66130c3c-8e25-46ae-8433-2c3a2853027b",
   "metadata": {},
   "source": [
    "## Custom transformation and manual output shape\n",
    "\n",
    "We also provide the option to use tonic transformations on the data. See the tonic documentation to see a complete list and usage. If you need to use custom transformation, the automatic shape determination would not work. In that case, you can specify the output shape manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fde088a-74c9-45dd-bee7-beca88caa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_x_dim(events):\n",
    "    events['x'] += 500\n",
    "    return events\n",
    "    \n",
    "\n",
    "transformations = Compose(\n",
    "    [\n",
    "        expand_x_dim,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Init Processes\n",
    "frame_input = PropheseeCamera(device=SEQUENCE_FILENAME_RAW,\n",
    "                              transformations=transformations,\n",
    "                              sensor_shape=(height, width),\n",
    "                              num_output_time_bins=1,\n",
    "                              out_shape=(1, 2, height, width+500))\n",
    "\n",
    "recv = VisProcess(shape=frame_input.s_out.shape)\n",
    "\n",
    "# Connect\n",
    "frame_input.s_out.connect(recv.s_in)\n",
    "\n",
    "# Run\n",
    "num_steps = 200\n",
    "run_cfg = Loihi2SimCfg()\n",
    "run_cnd = RunSteps(num_steps=num_steps)\n",
    "\n",
    "frame_input.run(condition=run_cnd, run_cfg=run_cfg)\n",
    "frame_input.stop()\n",
    "                              \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09284e-4f27-4aa0-98a8-6a8edc66cdcd",
   "metadata": {},
   "source": [
    "## Swipe detector\n",
    "\n",
    "In the next section, we will create a simple hand gesture recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f295a395-155b-4504-985f-28c1fd92bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Cam\n",
      "Create FF weights\n",
      "Create down weights\n",
      "Create up weights\n",
      "Create left weights\n",
      "Create right weights\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "transformations = Compose(\n",
    "    [\n",
    "        MirrorHorizontally(height=height),\n",
    "        MirrorVertically(width=width),\n",
    "        Downsample(factor=0.3),\n",
    "        MergePolarities(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Create Cam\")\n",
    "# Init Processes\n",
    "frame_input = PropheseeCamera(device=SEQUENCE_FILENAME_RAW,\n",
    "                              transformations=transformations,\n",
    "                              sensor_shape=(height, width),\n",
    "                              num_output_time_bins=1)\n",
    "\n",
    "num_neurons = np.prod(frame_input.s_out.shape)\n",
    "_, _, scaled_height, scaled_width = frame_input.s_out.shape\n",
    "flat_shape = (num_neurons, )\n",
    "\n",
    "\n",
    "w_ff = 1000\n",
    "w_rec = 5\n",
    "kernel_size = 5\n",
    "\n",
    "vth = 1000\n",
    "du = dv = 2000\n",
    "\n",
    "\n",
    "print(\"Create FF weights\")\n",
    "\n",
    "ff_weights = csr_matrix(np.eye(num_neurons) * w_ff)\n",
    "\n",
    "\n",
    "print(\"Create down weights\")\n",
    "rec_weights_down = np.zeros((num_neurons, num_neurons)).reshape((scaled_height, scaled_width, scaled_height, scaled_width))\n",
    "for i in range(scaled_height):\n",
    "    for j in range(scaled_width):\n",
    "        rec_weights_down[i+1:i+kernel_size, j-kernel_size:j+kernel_size, i, j-kernel_size:j+kernel_size] = w_rec\n",
    "rec_weights_down = csr_matrix(rec_weights_down.reshape((num_neurons, num_neurons)))\n",
    "\n",
    "print(\"Create up weights\")\n",
    "rec_weights_up = np.zeros((num_neurons, num_neurons)).reshape((scaled_height, scaled_width, scaled_height, scaled_width))\n",
    "for i in range(scaled_height):\n",
    "    for j in range(scaled_width):\n",
    "        rec_weights_up[i-kernel_size:i, j-kernel_size:j+kernel_size, i, j-kernel_size:j+kernel_size] = w_rec\n",
    "rec_weights_up = csr_matrix(rec_weights_up.reshape((num_neurons, num_neurons)))\n",
    "\n",
    "print(\"Create left weights\")\n",
    "rec_weights_left = np.zeros((num_neurons, num_neurons)).reshape((scaled_height, scaled_width, scaled_height, scaled_width))\n",
    "for i in range(scaled_height):\n",
    "    for j in range(scaled_width):\n",
    "        rec_weights_left[i-kernel_size:i+kernel_size, j-kernel_size:j, i-kernel_size:i+kernel_size, j] = w_rec\n",
    "\n",
    "rec_weights_left = csr_matrix(rec_weights_left.reshape((num_neurons, num_neurons)))\n",
    "\n",
    "print(\"Create right weights\")\n",
    "rec_weights_right = np.zeros((num_neurons, num_neurons)).reshape((scaled_height, scaled_width, scaled_height, scaled_width))\n",
    "for i in range(scaled_height):\n",
    "    for j in range(scaled_width):\n",
    "        rec_weights_right[i-kernel_size:i+kernel_size, j+1:j+kernel_size, i-kernel_size:i+kernel_size, j] = w_rec\n",
    "rec_weights_right = csr_matrix(rec_weights_right.reshape((num_neurons, num_neurons)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f2219f9-5dcb-4efa-869d-ed160672bcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create down processes\n",
      "Create up processes\n",
      "Create left processes\n",
      "Create right processes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Create down processes\")\n",
    "ff_down = Sparse(weights=ff_weights)\n",
    "rec_down = Sparse(weights=rec_weights_down)\n",
    "lif_down = LIF(shape=frame_input.shape,\n",
    "              du=du,\n",
    "              dv=dv,\n",
    "              vth=vth)\n",
    "\n",
    "print(\"Create up processes\")\n",
    "ff_up = Sparse(weights=ff_weights)\n",
    "rec_up = Sparse(weights=rec_weights_up)\n",
    "lif_up = LIF(shape=frame_input.shape,\n",
    "              du=du,\n",
    "              dv=dv,\n",
    "              vth=vth)\n",
    "\n",
    "print(\"Create left processes\")\n",
    "ff_left = Sparse(weights=ff_weights)\n",
    "rec_left = Sparse(weights=rec_weights_left)\n",
    "lif_left = LIF(shape=frame_input.shape,\n",
    "              du=du,\n",
    "              dv=dv,\n",
    "              vth=vth)\n",
    "\n",
    "print(\"Create right processes\")\n",
    "ff_right = Sparse(weights=ff_weights)\n",
    "rec_right = Sparse(weights=rec_weights_right)\n",
    "lif_right = LIF(shape=frame_input.shape,\n",
    "              du=du,\n",
    "              dv=dv,\n",
    "              vth=vth)\n",
    "\n",
    "recv = VisUpDownProcess(shape=lif_down.s_out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ae5d7f7-3a6e-41a3-83cd-76f6d411f652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect\n"
     ]
    }
   ],
   "source": [
    "print(\"Connect\")\n",
    "# Connect\n",
    "frame_input.s_out.connect(recv.frame_in)\n",
    "\n",
    "frame_input.s_out.flatten().connect(ff_down.s_in)\n",
    "ff_down.a_out.reshape(lif_down.a_in.shape).connect(lif_down.a_in)\n",
    "lif_down.s_out.flatten().connect(rec_down.s_in)\n",
    "rec_down.a_out.reshape(lif_down.a_in.shape).connect(lif_down.a_in)\n",
    "lif_down.s_out.connect(recv.down_in)\n",
    "lif_down.s_out.connect(recv.down_frame_in)\n",
    "\n",
    "frame_input.s_out.flatten().connect(ff_up.s_in)\n",
    "ff_up.a_out.reshape(lif_up.a_in.shape).connect(lif_up.a_in)\n",
    "lif_up.s_out.flatten().connect(rec_up.s_in)\n",
    "rec_up.a_out.reshape(lif_up.a_in.shape).connect(lif_up.a_in)\n",
    "lif_up.s_out.connect(recv.up_in)\n",
    "lif_up.s_out.connect(recv.up_frame_in)\n",
    "\n",
    "frame_input.s_out.flatten().connect(ff_left.s_in)\n",
    "ff_left.a_out.reshape(lif_left.a_in.shape).connect(lif_left.a_in)\n",
    "lif_left.s_out.flatten().connect(rec_left.s_in)\n",
    "rec_left.a_out.reshape(lif_left.a_in.shape).connect(lif_left.a_in)\n",
    "lif_left.s_out.connect(recv.left_in)\n",
    "lif_left.s_out.connect(recv.left_frame_in)\n",
    "\n",
    "frame_input.s_out.flatten().connect(ff_right.s_in)\n",
    "ff_right.a_out.reshape(lif_right.a_in.shape).connect(lif_right.a_in)\n",
    "lif_right.s_out.flatten().connect(rec_right.s_in)\n",
    "rec_right.a_out.reshape(lif_right.a_in.shape).connect(lif_right.a_in)\n",
    "lif_right.s_out.connect(recv.right_in)\n",
    "lif_right.s_out.connect(recv.right_frame_in)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecbca89b-6f3e-463a-a9a0-cf34b23600f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run \n"
     ]
    }
   ],
   "source": [
    "print(\"Run \")\n",
    "# Run\n",
    "num_steps = 2000\n",
    "run_cfg = Loihi2SimCfg(exception_proc_model_map={Dense: PyDenseModelBitAcc, Sparse: PySparseModelBitAcc, LIF: PyLifModelBitAcc})\n",
    "run_cnd = RunSteps(num_steps=num_steps)\n",
    "\n",
    "frame_input.run(condition=run_cnd, run_cfg=run_cfg)\n",
    "frame_input.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bfc999-892e-49d9-8d71-5b71bca3cf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ee404-29cf-4016-96bd-a85b9535adb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c48f1-c9ff-43ba-8074-4946f920cf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
