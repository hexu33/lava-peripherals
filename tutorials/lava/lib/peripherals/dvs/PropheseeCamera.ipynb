{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df6eded-4456-406f-b5b9-bab9bc7c687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pweidel/prophesee_2_venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/metavision_core_ml/preprocessing/viz.py:41: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _viz_events(events, img):\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "from lava.magma.core.run_configs import Loihi2SimCfg, Loihi2HwCfg\n",
    "from lava.magma.core.run_conditions import RunSteps, RunContinuous\n",
    "from lava.magma.core.decorator import implements, requires, tag\n",
    "from lava.magma.core.model.py.model import PyLoihiProcessModel\n",
    "from lava.magma.core.model.py.ports import PyOutPort, PyInPort\n",
    "from lava.magma.core.model.py.type import LavaPyType\n",
    "from lava.magma.core.process.ports.ports import OutPort, InPort\n",
    "from lava.magma.core.process.process import AbstractProcess\n",
    "from lava.magma.core.process.variable import Var\n",
    "from lava.magma.core.resources import CPU\n",
    "from lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\n",
    "from lava.proc.io.sink import RingBuffer\n",
    "\n",
    "from lava.lib.peripherals.dvs.process import PropheseeCamera\n",
    "from metavision_core.utils import get_sample\n",
    "from metavision_sdk_cv import TrailFilterAlgorithm, ActivityNoiseFilterAlgorithm\n",
    "from metavision_sdk_core import PolarityFilterAlgorithm\n",
    "\n",
    "from multiprocessing import Pipe\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import IPython\n",
    "\n",
    "\n",
    "from metavision_core.event_io import RawReader\n",
    "import tonic\n",
    "from tonic.transforms import Compose\n",
    "from metavision_ml.preprocessing import histo, histo_quantized\n",
    "from metavision_core.event_io import RawReader\n",
    "\n",
    "from lava.proc.embedded_io.spike import PyToNxAdapter, NxToPyAdapter\n",
    "from lava.proc.lif.process import LIF, LogConfig\n",
    "from lava.proc.dense.process import Dense\n",
    "from lava.proc.sparse.process import Sparse\n",
    "import logging\n",
    "\n",
    "from utils import VisProcess\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e456b29-9503-4546-adc7-37c8d511dcf3",
   "metadata": {},
   "source": [
    "\n",
    "## Download raw event recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602de9c0-4501-42b3-b604-589346b82b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file https://dataset.prophesee.ai/index.php/s/2j8xCyufDLhi7sd/download -> ./80_balls.raw\n",
      "...99%, 18 MB, 6792 KB/s, 2 seconds passed"
     ]
    }
   ],
   "source": [
    "SEQUENCE_FILENAME_RAW = \"80_balls.raw\"\n",
    "# if the file doesn't exist, it will be downloaded from Prophesee's public sample server\n",
    "get_sample(SEQUENCE_FILENAME_RAW)\n",
    "\n",
    "reader = RawReader(SEQUENCE_FILENAME_RAW)\n",
    "height, width = reader.get_size()\n",
    "del reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e72821b-418a-451d-a52d-1e33f96faccb",
   "metadata": {},
   "source": [
    "## Basic usage\n",
    "\n",
    "Initialize PropheseeCamera using the path to the recording, in order to use a live camera, replace the path with an empty string (\"\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325a861d-1f49-4e8c-9a4b-a5acd4e994fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function AbstractProcess.__del__ at 0x7fb857018dc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pweidel/lava-nc/lava/src/lava/magma/core/process/process.py\", line 221, in __del__\n",
      "    self.stop()\n",
      "  File \"/home/pweidel/lava-nc/lava/src/lava/magma/core/process/process.py\", line 412, in stop\n",
      "    if self.runtime:\n",
      "  File \"/home/pweidel/lava-nc/lava/src/lava/magma/core/process/process.py\", line 275, in runtime\n",
      "    return self._runtime\n",
      "AttributeError: 'VisProcess' object has no attribute '_runtime'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'sensor_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Init Processes\u001b[39;00m\n\u001b[1;32m      2\u001b[0m frame_input \u001b[38;5;241m=\u001b[39m PropheseeCamera(device\u001b[38;5;241m=\u001b[39mSEQUENCE_FILENAME_RAW,\n\u001b[1;32m      3\u001b[0m                               sensor_shape\u001b[38;5;241m=\u001b[39m(height, width),\n\u001b[1;32m      4\u001b[0m                               num_output_time_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m recv \u001b[38;5;241m=\u001b[39m \u001b[43mVisProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Connect\u001b[39;00m\n\u001b[1;32m      9\u001b[0m frame_input\u001b[38;5;241m.\u001b[39ms_out\u001b[38;5;241m.\u001b[39mconnect(recv\u001b[38;5;241m.\u001b[39ms_in)\n",
      "File \u001b[0;32m~/lava-nc/lava/src/lava/magma/core/process/process.py:32\u001b[0m, in \u001b[0;36mProcessPostInitCaller.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 32\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_post_init\u001b[39m\u001b[38;5;124m\"\u001b[39m)()\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'sensor_size'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Init Processes\n",
    "frame_input = PropheseeCamera(device=SEQUENCE_FILENAME_RAW,\n",
    "                              sensor_shape=(height, width),\n",
    "                              num_output_time_bins=1)\n",
    "\n",
    "recv = VisProcess(shape=frame_input.s_out.shape[2:])\n",
    "\n",
    "# Connect\n",
    "frame_input.s_out.connect(recv.s_in)\n",
    "\n",
    "# Run\n",
    "num_steps = 200\n",
    "run_cfg = Loihi2SimCfg()\n",
    "run_cnd = RunSteps(num_steps=num_steps)\n",
    "\n",
    "frame_input.run(condition=run_cnd, run_cfg=run_cfg)\n",
    "frame_input.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a80a0-3d5f-4317-9a95-14b121bbf1d5",
   "metadata": {},
   "source": [
    "## Apply filters\n",
    "\n",
    "As you see, the falling items in the recording have a trail. In order to remove the trails, we can apply the TrailFilterAlgorithm from the metavision_sdk. See the metavision documentation for more filters and their usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0d0587-677e-4dae-8e6e-f40b64098e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [TrailFilterAlgorithm(width=width, height=height, threshold=100000),\n",
    "           ActivityNoiseFilterAlgorithm(width=width, height=height, threshold=1000),]\n",
    "\n",
    "\n",
    "# Init Processes\n",
    "frame_input = PropheseeCamera(device=SEQUENCE_FILENAME_RAW,\n",
    "                              filters=filters,\n",
    "                              sensor_shape=(height, width),\n",
    "                              num_output_time_bins=1)\n",
    "\n",
    "recv = VisProcess(shape=frame_input.s_out.shape)\n",
    "\n",
    "# Connect\n",
    "frame_input.s_out.connect(recv.s_in)\n",
    "\n",
    "# Run\n",
    "num_steps = 200\n",
    "run_cfg = Loihi2SimCfg()\n",
    "run_cnd = RunSteps(num_steps=num_steps)\n",
    "\n",
    "frame_input.run(condition=run_cnd, run_cfg=run_cfg)\n",
    "frame_input.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebade859-eaca-4b07-a6df-3ecf4136df62",
   "metadata": {},
   "source": [
    "## Apply transformations\n",
    "\n",
    "We also provide the option to use tonic transformations on the data as for example downsampling. See the tonic documentation to see a complete list and usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8c5acd-4745-43f5-9efa-38087d2cc710",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = tonic.transforms.Compose(\n",
    "    [\n",
    "        tonic.transforms.Downsample(spatial_factor=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Init Processes\n",
    "frame_input = PropheseeCamera(device=SEQUENCE_FILENAME_RAW,\n",
    "                              transformations=transformations,\n",
    "                              sensor_shape=(height, width),\n",
    "                              num_output_time_bins=1)\n",
    "\n",
    "recv = VisProcess(shape=frame_input.s_out.shape)\n",
    "\n",
    "# Connect\n",
    "frame_input.s_out.connect(recv.s_in)\n",
    "\n",
    "# Run\n",
    "num_steps = 200\n",
    "run_cfg = Loihi2SimCfg()\n",
    "run_cnd = RunSteps(num_steps=num_steps)\n",
    "\n",
    "frame_input.run(condition=run_cnd, run_cfg=run_cfg)\n",
    "frame_input.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66130c3c-8e25-46ae-8433-2c3a2853027b",
   "metadata": {},
   "source": [
    "## Custom transformation and manual output shape\n",
    "\n",
    "If you need to write your own transformation, the automatic shape determination would not work. In that case, you can specify the output shape manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31db0ab5-93b6-4809-86ba-c637fb7f09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_x_dim(events):\n",
    "    events['x'] += 500\n",
    "    return events\n",
    "    \n",
    "\n",
    "transformations = tonic.transforms.Compose(\n",
    "    [\n",
    "        expand_x_dim,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Init Processes\n",
    "frame_input = PropheseeCamera(device=SEQUENCE_FILENAME_RAW,\n",
    "                              transformations=transformations,\n",
    "                              sensor_shape=(height, width),\n",
    "                              num_output_time_bins=1,\n",
    "                              out_shape=(1, 2, height, width+500))\n",
    "\n",
    "recv = VisProcess(shape=frame_input.s_out.shape)\n",
    "\n",
    "# Connect\n",
    "frame_input.s_out.connect(recv.s_in)\n",
    "\n",
    "# Run\n",
    "num_steps = 200\n",
    "run_cfg = Loihi2SimCfg()\n",
    "run_cnd = RunSteps(num_steps=num_steps)\n",
    "\n",
    "frame_input.run(condition=run_cnd, run_cfg=run_cfg)\n",
    "frame_input.stop()\n",
    "                              \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c4906-ef8c-40cc-85a3-68e58e4aeb77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
